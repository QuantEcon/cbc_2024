---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Endogenous Grid Method

----

#### John Stachurski
#### Prepared for the CBC Computational Workshop (May 2024)

----

## Overview

In this lecture we use the endogenous grid method (EGM) to solve a basic optimal savings problem.

We provide both Numba and JAX versions.


Uncomment if necessary:

```{code-cell} ipython3
#!pip install --upgrade quantecon
```

Let's run some imports

```{code-cell} ipython3
import quantecon as qe
from collections import namedtuple
import matplotlib.pyplot as plt
import numpy as np
import jax
import jax.numpy as jnp
import numba
```

What GPU are we running?

```{code-cell} ipython3
!nvidia-smi
```

We use 64 bit floating point numbers for extra precision.

```{code-cell} ipython3
jax.config.update("jax_enable_x64", True)
```

## Setup 

Consider a household that chooses $\{c_t\}_{t \geq 0}$ to maximize

$$
    \mathbb{E} \, \sum_{t=0}^{\infty} \beta^t u(c_t)
$$

subject to

$$
    a_{t+1} = R(a_t - c_t)  + Y_{t+1}
    \quad \text{and}
    \quad 0 \leq c_t \leq a_t \quad \text{for } t \geq 0
$$

Here

* $\beta \in (0,1)$ is a discount factor
* $R = 1 + r$ where $r$ is the interest rate
* the income process $\{Y_t\}$ is a generated by stochastic matrix $P$

The matrix $P$ and the grid of values taken by $Y_t$ are obtained by discretizing the AR(1) process

$$
    Y_{t+1} = \rho Y_t + \nu \epsilon_{t+1}
$$

where $\{\epsilon_t\}$ is IID and standard normal.

Utility has the CRRA specification

$$
    u(c) = \frac{c^{1 - \gamma}} {1 - \gamma}
$$

We start with a namedtuple to store parameters and arrays

```{code-cell} ipython3
Model = namedtuple('Model', ('β', 'R', 'γ', 's_grid', 'y_grid', 'P'))
```

The following function stores default parameter values for the income fluctuation problem and creates suitable arrays.

```{code-cell} ipython3
def ifp(R=1.01,             # Gross interest rate
        β=0.99,             # Discount factor
        γ=1.5,              # CRRA preference parameter
        s_max=16,           # Savings grid max
        s_size=200,         # Savings grid size
        ρ=0.99,             # Income persistence
        ν=0.02,             # Income volatility
        y_size=25):         # Income grid size
  
    # require R β < 1 for convergence
    assert R * β < 1, "Stability condition failed."

    # Create arrays
    mc = qe.tauchen(y_size, ρ, ν)
    y_grid, P = jnp.exp(mc.state_values), jnp.array(mc.P)
    s_grid = jnp.linspace(0, s_max, s_size)

    # Pack and return
    return Model(β, R, γ, s_grid, y_grid, P)
```

### Solution method

Let $S = \mathbb R_+ \times \mathsf Y$ be the set of possible values for the state $(a_t, Y_t)$.

We aim to compute an optimal consumption policy $\sigma^* \colon S \to \mathbb R$, under which dynamics are given by

$$
    c_t = \sigma^*(a_t, Y_t)
    \quad \text{and} \quad
    a_{t+1} = R (a_t - c_t) + Y_{t+1}
$$


We solve for this policy via the endogenous grid method (EGM).

+++

### Euler equation

The Euler equation for the optimization problem is

$$
    u' (c_t)
    = \max \left\{
        \beta R \,  \mathbb{E}_t  u'(c_{t+1})  \,,\;  u'(a_t)
    \right\}
$$

(An explanation for this expression can be found [here](https://python.quantecon.org/ifp.html#value-function-and-euler-equation).)

We rewrite the Euler equation in functional form

$$
    (u' \circ \sigma)  (a, y)
    = \max \left\{
    \beta R \, \mathbb E_y (u' \circ \sigma)
        [R (a - \sigma(a, y)) + \hat Y, \, \hat Y]
    \, , \;
         u'(a)
         \right\}
$$


where $(u' \circ \sigma)(a, y) := u'(\sigma(a, y))$ and $\sigma$ is a consumption
policy.

+++

Let's set up an operator $K$ that updates policies and coverges to an optimal
consumption policy.

Fixing policy $\sigma$, we define $(K \sigma) (a,y)$ as the unique $c \in [0, a]$ that solves

$$
u'(c)
= \max \left\{
           \beta R \, \mathbb E_y (u' \circ \sigma) \,
           [R (a - c) + \hat Y, \, \hat Y]
           \, , \;
           u'(a)
     \right\}
$$ 

It [can be shown that](https://python.quantecon.org/ifp.html)

1. iterating with $K$ computes an optimal policy and
2. if $\sigma$ is increasing in its first argument, then so is $K\sigma$


### EGM

EGM is a technique for computing $K\sigma$ given $\sigma$ along a grid of asset values.

Notice that, since $u'(a) \to \infty$ as $a \downarrow 0$, 

* the second term in the max above dominates for sufficiently small $a$.
* We have $c=a$ for all such $a$.

Hence, for sufficiently small $a$,

$$
   u'(a) \geq
   \beta R \, \mathbb E_y (u' \circ \sigma) \,
           [\hat Y, \, \hat Y]
$$

Equality holds at $\bar a(y)$ given by 

$$
   \bar a (y) =
   (u')^{-1}
   \left\{
       \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [\hat Y, \, \hat Y]
   \right\}
$$

We can now write

$$
u'(c)
    = \begin{cases}
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a - c) + \hat Y, \, \hat Y]
               & \text{if } a > \bar a (y) \\
        u'(a)  & \text{if } a \leq \bar a (y)
    \end{cases}
$$

Equivalently, we can state that the $c$ satisfying $c = (K\sigma)(a, y)$ obeys

$$
c = \begin{cases}
        (u')^{-1}
        \left\{
            \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a - c) + \hat Y, \, \hat Y]
        \right\}
               & \text{if } a > \bar a (y) \\
            a  & \text{if } a \leq \bar a (y)
    \end{cases}
$$ 

We begin with an *exogenous* grid of saving values $0 = s_0 < \ldots < s_{N-1}$

Using the exogenous savings grid, and a fixed value of $y$, we create an *endogenous* asset grid
$a_0, \ldots, a_{N-1}$ and a consumption grid $c_0, \ldots, c_{N-1}$ as follows.

First we set $a_0 = c_0 = 0$, since zero consumption is an optimal (in fact the only) choice when $a=0$.

Then, for $i > 0$, we compute 

$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R s_i + \hat Y, \, \hat Y]
     \right\}
     \quad \text{for all } i
$$ 

and we set 

$$
    a_i = s_i + c_i 
$$ 

Since $s_i > 0$, choosing $c_i$ as above gives

$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R s_i + \hat Y, \, \hat Y]
     \right\}
     \geq \bar a(y)
$$

where the inequality uses the fact that $\sigma$ is increasing in its first argument.

If we now take $a_i = s_i + c_i$ we get $a_i > \bar a(y)$, so the pair $(a_i, c_i)$ satisfies

$$
    c_i
    = (u')^{-1}
    \left\{ 
        \beta R \, \mathbb E_y (u' \circ \sigma) \,
               [R (a_i - c_i) + \hat Y, \, \hat Y]
     \right\}
     \quad \text{and} \quad a_i > \bar a(y)
$$

Thus, we have computed $K\sigma(a_i, y)$.

Repeating over all $i$ allows us to calculate $K\sigma$ at all (endogenous) grid points.

We are now ready to iterate with $K$.

+++

## JAX version 

First we define a vectorized operator $K$ based on the EGM.

Notice in the code below that 

* we avoid all loops by vectorization
* the function is pure (no globals, no mutation of inputs)

```{code-cell} ipython3
@jax.jit
def K_egm_jax(a_vec, σ, model):
    "The vectorized operator K using EGM."
    
    # Unpack
    β, R, γ, s_grid, y_grid, P = model
    s_size, y_size = len(s_grid), len(y_grid)
    
    def u_prime(c):
        return c**(-γ)

    def u_prime_inv(u):
            return u**(-1/γ)

    # Linearly interpolate σ(a, y)
    def σ_f(a, y):
        return jnp.interp(a, a_vec[:, y], σ[:, y])
    σ_vec = jnp.vectorize(σ_f)

    # Broadcast and vectorize
    y_hat = jnp.reshape(y_grid, (1, 1, y_size))
    y_hat_idx = jnp.reshape(jnp.arange(y_size), (1, 1, y_size))
    s = jnp.reshape(s_grid, (s_size, 1, 1))
    P = jnp.reshape(P, (1, y_size, y_size))
    
    # Evaluate consumption choice
    a_next = R * s + y_hat
    σ_next = σ_vec(a_next, y_hat_idx)
    up = u_prime(σ_next)
    E = jnp.sum(up * P, axis=-1)
    c = u_prime_inv(β * R * E)

    # Set up a column vector with zero in the first row and ones elsewhere
    e_0 = jnp.ones(s_size) - jnp.identity(s_size)[:, 0]
    e_0 = jnp.reshape(e_0, (s_size, 1))

    # The policy is computed consumption with the first row set to zero
    σ_out = c * e_0

    # Compute a_out by a = s + c
    a_out = np.reshape(s_grid, (s_size, 1)) + σ_out
    
    return a_out, σ_out
```

Next we define a successive approximator that repeatedly applies $K$.

```{code-cell} ipython3
def successive_approx_jax(model,        
                          tol=1e-5,
                          max_iter=100_000,
                          verbose=True,
                          print_skip=25):

    # Unpack
    β, R, γ, s_grid, y_grid, P = model
    s_size, y_size = len(s_grid), len(y_grid)
    
    # Initial condition is to consume all in every state
    σ_init = jnp.repeat(s_grid, y_size)
    σ_init = jnp.reshape(σ_init, (s_size, y_size))
    a_init = jnp.copy(σ_init)
    a_vec, σ_vec = a_init, σ_init
    
    i = 0
    error = tol + 1

    while i < max_iter and error > tol:
        a_new, σ_new = K_egm_jax(a_vec, σ_vec, model)
        error = jnp.max(jnp.abs(σ_vec - σ_new))
        i += 1
        if verbose and i % print_skip == 0:
            print(f"Error at iteration {i} is {error}.")
        a_vec, σ_vec = a_new, σ_new

    if error > tol:
        print("Failed to converge!")
    else:
        print(f"\nConverged in {i} iterations.")

    return a_new, σ_new
```

### Numba version 

Below we provide a second set of code, which solves the same model with Numba.

The purpose of this code is to cross-check our results from the JAX version, as
well as to do a runtime comparison.

Most readers will want to skip ahead to the next section, where we solve the
model and run the cross-check.

```{code-cell} ipython3
@numba.jit
def K_egm_nb(a_vec, σ, model):
    "The operator K using Numba."

    # Unpack
    β, R, γ, s_grid, y_grid, P = model
    s_size, y_size = len(s_grid), len(y_grid)

    def u_prime(c):
        return c**(-γ)

    def u_prime_inv(u):
        return u**(-1/γ)

    # Linear interpolation of policy using endogenous grid
    def σ_f(a, z):
        return np.interp(a, a_vec[:, z], σ[:, z])
    
    # Allocate memory for new consumption array
    σ_out = np.zeros_like(σ)
    a_out = np.zeros_like(σ_out)
    
    for i, s in enumerate(s_grid[1:]):
        i += 1
        for z in range(y_size):
            expect = 0.0
            for z_hat in range(y_size):
                expect += u_prime(σ_f(R * s + y_grid[z_hat], z_hat)) * \
                            P[z, z_hat]
            c = u_prime_inv(β * R * expect)
            σ_out[i, z] = c
            a_out[i, z] = s + c
    
    return a_out, σ_out
```

```{code-cell} ipython3
def successive_approx_numba(model,        # Class with model information
                            tol=1e-5,
                            max_iter=100_000,
                            verbose=True,
                            print_skip=25):

    # Unpack
    β, R, γ, s_grid, y_grid, P = model
    s_size, y_size = len(s_grid), len(y_grid)

    # make NumPy versions of arrays
    s_grid, y_grid, P = [np.array(x) for x in (s_grid, y_grid, P)]
    
    σ_init = np.repeat(s_grid, y_size)
    σ_init = np.reshape(σ_init, (s_size, y_size))
    a_init = np.copy(σ_init)
    a_vec, σ_vec = a_init, σ_init
    
    # Set up loop
    i = 0
    error = tol + 1

    while i < max_iter and error > tol:
        a_new, σ_new = K_egm_nb(a_vec, σ_vec, model)
        error = np.max(np.abs(σ_vec - σ_new))
        i += 1
        if verbose and i % print_skip == 0:
            print(f"Error at iteration {i} is {error}.")
        a_vec, σ_vec = a_new, σ_new

    if error > tol:
        print("Failed to converge!")
    else:
        print(f"\nConverged in {i} iterations.")

    return a_new, σ_new
```

## Solutions

Here we solve the IFP with JAX and Numba.

We will compare both the outputs and the execution time.

### Outputs

```{code-cell} ipython3
model = ifp()
```

Here's a first run of the JAX code.

```{code-cell} ipython3
a_star_jax, σ_star_jax = successive_approx_jax(model,
                                               print_skip=100)
```

Next let's solve the same IFP with Numba.

```{code-cell} ipython3
a_star_nb, σ_star_nb = successive_approx_numba(model,
                                                print_skip=100)
```

Now let's check the outputs in a plot to make sure they are the same.

```{code-cell} ipython3
β, R, γ, s_grid, y_grid, P = model
s_size, y_size = len(s_grid), len(y_grid)

fig, ax = plt.subplots()

for z in (0, y_size-1):
    ax.plot(a_star_nb[:, z], 
            σ_star_nb[:, z], 
            '--', lw=2,
            label=f"Numba EGM: consumption when $z={z}$")
    ax.plot(a_star_jax[:, z], 
            σ_star_jax[:, z], 
            label=f"JAX EGM: consumption when $z={z}$")

ax.set_xlabel('asset')
plt.legend()
plt.show()
```

### Timing

Now let's compare execution time of the two methods

```{code-cell} ipython3
qe.tic()
a_star_jax, σ_star_jax = successive_approx_jax(model,
                                         print_skip=1000)
jax_time = qe.toc()
```

```{code-cell} ipython3
qe.tic()
a_star_nb, σ_star_nb = successive_approx_numba(model,
                                         print_skip=1000)
numba_time = qe.toc()
```

How much faster is JAX?

```{code-cell} ipython3
numba_time / jax_time
```

The JAX code is significantly faster, as expected.

This difference will increase when more features (and state variables) are added
to the model.

+++

## Exercise

Try replacing `successive_approx_jax` with a jitted version (`@jax.jit` at the top) that uses `jax.lax.while_loop`.

Measure the execution time (after running once to compile) and compare it with the timings above.

Also plot the resulting functions using the plotting code above to make sure that you're still getting the same outputs.

```{code-cell} ipython3
#Put your code here
```

```{code-cell} ipython3
for i in range(18):
    print("Solution below! 🐘")
```

```{code-cell} ipython3
@jax.jit
def successive_approx_jax_jitted(
                          model,        
                          tol=1e-5,
                          max_iter=100_000,
                          verbose=True,
                          print_skip=25):

    # Unpack
    β, R, γ, s_grid, y_grid, P = model
    s_size, y_size = len(s_grid), len(y_grid)
    
    # Initial condition is to consume all in every state
    σ_init = jnp.repeat(s_grid, y_size)
    σ_init = jnp.reshape(σ_init, (s_size, y_size))
    a_init = jnp.copy(σ_init)

    def update(state):
        i, a_vec, σ_vec, error = state
        a_new, σ_new = K_egm_jax(a_vec, σ_vec, model) 
        error = jnp.max(jnp.abs(σ_vec - σ_new))
        i += 1
        return i, a_new, σ_new, error

    def condition(state):
        i, a_vec, σ_vec, error = state
        return jnp.logical_and(i < max_iter, error > tol)

    init_state = (0, a_init, σ_init, tol + 1)
    state = jax.lax.while_loop(condition, update, init_state)

    return state
```

Here's a first run.

```{code-cell} ipython3

i, a_star_jax_jit, σ_star_jax_jit, error = successive_approx_jax_jitted(model,
                                                     print_skip=1000)
```

```{code-cell} ipython3
print(f"Run completed in {i} iterations with error {error:.5}.")
```

Now let's time it.

```{code-cell} ipython3
qe.tic()
i, a_star_jax_jit, σ_star_jax_jit, error = successive_approx_jax_jitted(model,
                                                     print_skip=1000)
jax_jit_time = qe.toc()
```

```{code-cell} ipython3
jax_time / jax_jit_time
```

```{code-cell} ipython3
numba_time / jax_jit_time
```

```{code-cell} ipython3
β, R, γ, s_grid, y_grid, P = model
s_size, y_size = len(s_grid), len(y_grid)

fig, ax = plt.subplots()

for z in (0, y_size-1):
    ax.plot(a_star_jax_jit[:, z], 
            σ_star_jax_jit[:, z], 
            label=f"JAX EGM: consumption when $z={z}$")

ax.set_xlabel('asset')
plt.legend()
plt.show()
```

```{code-cell} ipython3

```
