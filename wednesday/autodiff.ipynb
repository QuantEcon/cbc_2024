{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390c9e2d",
   "metadata": {},
   "source": [
    "# Adventures with Autodiff\n",
    "\n",
    "------\n",
    "\n",
    "#### Prepared for the CBC Workshop May 2024\n",
    "#### John Stachurski\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c7c33",
   "metadata": {},
   "source": [
    "## What is automatic differentiation\n",
    "\n",
    "Autodiff is a technique for calculating derivatives on a computer.\n",
    "\n",
    "### Autodiff is not finite differences\n",
    "\n",
    "The derivative of the exponential function $f(x) = \\exp(2x)$ function is\n",
    "\n",
    "$$\n",
    "    (\\exp(2 x))' = 2 \\exp(2x)\n",
    "$$\n",
    "\n",
    "so the derivative at 0 is 2.\n",
    "\n",
    "\n",
    "A computer that doesn't know how to take derivatives might approximate this with finite approximation\n",
    "\n",
    "$$\n",
    "    f_h(x) := \\frac{f(x+h) - f(x)}{h}\n",
    "    \\quad \\text{for some small } h > 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e879439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.exp(2 * x)\n",
    "\n",
    "def fh(x, h=0.1):\n",
    "    return (f(x + h) - f(x))/h\n",
    "\n",
    "x_grid = np.linspace(-2, 2, 200)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f(x_grid), label=\"$f$\")\n",
    "ax.plot(x_grid, f(1.0) + fh(1.0) * (x_grid - 1.0), label=\"approximation\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bbd59",
   "metadata": {},
   "source": [
    "This kind of numerical derivative is often inaccurate and unstable.\n",
    "\n",
    "One reason is that in the calculation of\n",
    "\n",
    "$$\n",
    "    \\frac{f(x+h) - f(x)}{h} \n",
    "$$\n",
    "\n",
    "we often have very small numbers in the numerator and denominator, which causes rounding errors.\n",
    "\n",
    "The situation is exponentially worse in high dimensions, working with higher order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf47fe",
   "metadata": {},
   "source": [
    "### Autodiff is not symbolic algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, init_printing\n",
    "init_printing(use_unicode=True)\n",
    "\n",
    "m, a, b, x = symbols('m a b x')\n",
    "expr = (a*x + b)**m\n",
    "expr.diff((x, 6))  # 6-th order derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7bd09",
   "metadata": {},
   "source": [
    "Symbolic calculus tries to use rules for differentiation to produce a single\n",
    "closed-form expression representing a derivative.\n",
    "\n",
    "This is useful at times but has disadvantages when considering high performance\n",
    "computing.\n",
    "\n",
    "One disadvantage is that it cannot differentiate through control flow, only through mathematical expressions.\n",
    "\n",
    "Also, using symbolic calculus might involve many redundant calculations.\n",
    "\n",
    "For example, consider\n",
    "\n",
    "$$\n",
    "    (f g h)'\n",
    "    = (f' g + g' f) h + (f g) h'\n",
    "$$\n",
    "\n",
    "If we evaluate at $x$, then we evalute $f(x)$ and $g(x)$ twice each.\n",
    "\n",
    "Also, computing $f'(x)$ and $f(x)$ might involve similar terms (e.g., $(\\exp(2x)' = 2 \\exp(2x)$) but this is not exploited in symbolic algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b551722d",
   "metadata": {},
   "source": [
    "### Autodiff \n",
    "\n",
    "Autodiff attempts to evaluate expressions numerically (using the rules of\n",
    "calculus) at each stage, rather than manipulating large symbolic expressions.\n",
    "\n",
    "Autodiff produces functions that evaluates derivatives at numerical values\n",
    "passed in by the calling code, rather than producing a single symbolic\n",
    "expression representing the entire derivative.\n",
    "\n",
    "Derivatives are deconstructed into component parts via the chain rule.\n",
    "\n",
    "The chain rule is applied until the point where the terms reduce to primitive functions that the program knows how to differentiate exactly (addition, subtraction, exponentiation, sine and cosine, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694d403",
   "metadata": {},
   "source": [
    "## Some experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3757d",
   "metadata": {},
   "source": [
    "Let's start with some real-valued functions on $\\mathbb R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666deceb",
   "metadata": {},
   "source": [
    "### A differentiable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.sin(x) - 2 * jnp.cos(3 * x) * jnp.exp(- x**2)\n",
    "\n",
    "x_grid = jnp.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)\n",
    "f_prime_vec = jax.vmap(f_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8973a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = f(x_grid)\n",
    "f_prime_vals = f_prime_vec(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = f(x_grid)\n",
    "f_prime_vals = f_prime_vec(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_vals, label=\"$f$\")\n",
    "ax.plot(x_grid, f_prime_vals, label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c8034",
   "metadata": {},
   "source": [
    "### Absolute value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab52c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return jnp.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2cba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)\n",
    "f_prime_vec = jax.vmap(f_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vals = f(x_grid)\n",
    "f_prime_vals = f_prime_vec(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_vals, label=\"$f$\")\n",
    "ax.plot(x_grid, f_prime_vals, label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a3d64",
   "metadata": {},
   "source": [
    "### Differentiating through control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0674c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x < 0:\n",
    "        for i in range(3):\n",
    "            x *= x\n",
    "    else:\n",
    "        x = sum((x**i + i) for i in range(5))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    def f1(x):\n",
    "        for i in range(2):\n",
    "            x *= 0.2 * x\n",
    "        return x\n",
    "    def f2(x):\n",
    "        x = sum((x**i + i) for i in range(3))\n",
    "        return x\n",
    "    y = jax.lax.cond(x < 0, f1, f2, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vec = jax.vmap(f)\n",
    "f_prime = jax.grad(f)\n",
    "f_prime_vec = jax.vmap(f_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = jnp.linspace(-5, 5, 100)\n",
    "f_vals = f_vec(x_grid)\n",
    "f_prime_vals = f_prime_vec(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_vals, label=\"$f$\")\n",
    "ax.plot(x_grid, f_prime_vals, label=\"$f'$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f7a25",
   "metadata": {},
   "source": [
    "### Differentiating through a linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, xp):\n",
    "    yp = jnp.cos(2 * xp)\n",
    "    return jnp.interp(x, xp, yp)\n",
    "\n",
    "xp = jnp.linspace(-5, 5, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40042205",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f(x_grid, xp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = jax.grad(f)\n",
    "f_prime_vec = jax.vmap(f_prime, (0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_grid, f_prime_vec(x_grid, xp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc7f47",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecfe526",
   "metadata": {},
   "source": [
    "Let's try implementing a simple version of gradient descent.\n",
    "\n",
    "As a (very contrived) application, we'll use gradient descent to solve for the OLS parameter estimates in simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389db0b6",
   "metadata": {},
   "source": [
    "### Simulated data\n",
    "\n",
    "Let's generate some simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc95259",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "key = jax.random.PRNGKey(1234)\n",
    "x = jax.random.uniform(key, (n,))\n",
    "\n",
    "α, β = 0.5, 1.0  # Set the true intercept and slope.\n",
    "key, subkey = jax.random.split(key)\n",
    "y = α * x + β + 0.1 * jax.random.normal(subkey, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86834593",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = x.mean()\n",
    "my = y.mean()\n",
    "α_hat = jnp.sum((x - mx) * (y - my)) / jnp.sum((x - mx)**2)\n",
    "β_hat = my - α_hat * mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263dccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "α_hat, β_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fe274",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, α_hat * x + β_hat, 'k-')\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe485b80",
   "metadata": {},
   "source": [
    "### A function for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(f,       # Function to be minimized\n",
    "                 args,    # Extra arguments to the function\n",
    "                 p0,      # Initial condition\n",
    "                 λ=0.1,   # Initial learning rate\n",
    "                 tol=1e-5, \n",
    "                 max_iter=1_000):\n",
    "    \n",
    "    f_grad = jax.grad(f)\n",
    "    p = jnp.array(p0)\n",
    "    df = f_grad(p, args)\n",
    "    ϵ = tol + 1\n",
    "    i = 0\n",
    "    while ϵ > tol and i < max_iter:\n",
    "        new_p = p - λ * df\n",
    "        new_df = f_grad(new_p, args)\n",
    "        Δp = new_p - p\n",
    "        Δdf = new_df - df\n",
    "        λ = jnp.abs(Δp @ Δdf) / (Δdf @ Δdf)\n",
    "        ϵ = jnp.max(jnp.abs(Δp))\n",
    "        p, df = new_p, new_df\n",
    "        i += 1\n",
    "        \n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77771f70",
   "metadata": {},
   "source": [
    "### Minimizing squared loss by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46dfca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, data):\n",
    "    a, b = params\n",
    "    x, y = data\n",
    "    return jnp.sum((y - a * x - b)**2)\n",
    "\n",
    "p0 = (1.0, 1.0)\n",
    "data = x, y\n",
    "α_hat, β_hat = grad_descent(loss, data, p0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_grid = jnp.linspace(0, 1, 100)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, α_hat * x_grid + β_hat, 'k-', alpha=0.6)\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c847383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, data):\n",
    "    a, b, c = params\n",
    "    x, y = data\n",
    "    return jnp.sum((y - a * x**2 - b * x - c)**2)\n",
    "\n",
    "p0 = jnp.ones(3)\n",
    "α_hat, β_hat, γ_hat = grad_descent(loss, data, p0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, α_hat * x_grid**2 + β_hat * x_grid + γ_hat, 'k-', alpha=0.6)\n",
    "ax.text(0.1, 1.55, rf'$\\hat \\alpha = {α_hat:.3}$')\n",
    "ax.text(0.1, 1.50, rf'$\\hat \\beta = {β_hat:.3}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499281dd",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write a function called `poly` that with signature `poly(x, params)` that\n",
    "computes the value of a polynomial at $x \\in \\mathbb R$.\n",
    "\n",
    "The array `params` is the vector of polynomial coefficients.\n",
    "\n",
    "For example, if `params = p0, p1, p2`, then the function returns\n",
    "\n",
    "$$\n",
    "    p_0 + p_1 x + p_2 x^2\n",
    "$$\n",
    "\n",
    "Once you have this function working, use it for polynomial regression.\n",
    "\n",
    "The (empirical) loss becomes\n",
    "\n",
    "$$\n",
    "    \\ell(p, x, y) \n",
    "    = \\sum_{i=1}^n (y_i = f(x_i; p))^2\n",
    "$$\n",
    "\n",
    "where $f(x_i, p)$ is the polynomial with coefficient vector $p$ evaluated at point $x_i$.\n",
    "\n",
    "Set $k=6$ and set the initial guess of `params` to `jnp.ones(k)`.\n",
    "\n",
    "Use gradient descent to find the array `params` that minimizes the loss\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(\"Solution below 🙈🦀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d23c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x, params):\n",
    "    k = len(params)\n",
    "    x_powers = jnp.full(k-1, x)  # x, x^2, x^3, ..., x^{n-1}\n",
    "    return params[0] + jnp.cumprod(x_powers) @ params[1:] \n",
    "\n",
    "# Vectorize the function in x\n",
    "poly = jax.vmap(poly, (0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, data):\n",
    "    x, y = data\n",
    "    return jnp.sum((y - poly(x, params))**2)\n",
    "\n",
    "k = 6\n",
    "p0 = jnp.ones(k)\n",
    "p_hat = grad_descent(loss, data, p0)\n",
    "print('Estimated parameter vector:')\n",
    "print(p_hat)\n",
    "print('\\n\\n')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_grid, poly(x_grid, p_hat), 'k-', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d4654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5e347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
