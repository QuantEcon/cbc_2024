{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68489c6",
   "metadata": {},
   "source": [
    "# Finite Markov Chains\n",
    "\n",
    "\n",
    "## Topics\n",
    "\n",
    "- Definitions\n",
    "- Simulation\n",
    "- Marginal Distributions\n",
    "- Irreducibility and Aperiodicity\n",
    "- Stationary Distributions\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Markov chains are one of the most useful classes of stochastic processes.\n",
    "\n",
    "They are flexible and supported by many elegant theoretical results, as well as being central to quantitative modeling.\n",
    "\n",
    "In this lecture, we \n",
    "\n",
    "* review some of the theory of Markov chains.\n",
    "* implement key ideas in Python\n",
    "* introduce some of the routines for working with Markov chains available in [QuantEcon.py](https://quantecon.org/quantecon-py/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quantecon  # Uncomment if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b885",
   "metadata": {},
   "source": [
    "Letâ€™s start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f43795",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "We recall some elementary definitions.\n",
    "\n",
    "\n",
    "### Stochastic Matrices\n",
    "\n",
    "\n",
    "A **stochastic matrix** (or **Markov matrix**)  is an $ n \\times n $ square matrix $ P $\n",
    "such that\n",
    "\n",
    "1. each element is nonnegative and  \n",
    "1. each row sums to one.\n",
    "\n",
    "\n",
    "Each row of $ P $ can be regarded as a probability mass function over $ n $ possible outcomes.\n",
    "\n",
    "It is simple to check that if $ P $ is a stochastic matrix, then so is the $ k $-th power $ P^k $ for all $ k \\in \\mathbb N $.\n",
    "\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "Let $ S $ be a finite set with $ n $ elements $ \\{x_1, \\ldots, x_n\\} $.\n",
    "\n",
    "We call $ S $ the **state space**.\n",
    "\n",
    "A **Markov chain** $ \\{X_t\\} $ on $ S $ is a sequence of random variables on $ S $ that have the **Markov property**.\n",
    "\n",
    "This means that, for any date $ t $ and any state $ y \\in S $,\n",
    "\n",
    "\n",
    "$$\n",
    "    \\mathbb P \\{ X_{t+1} = y  \\,|\\, X_t \\}\n",
    "    = \\mathbb P \\{ X_{t+1}  = y \\,|\\, X_t, X_{t-1}, \\ldots X_0\\} \n",
    "$$\n",
    "\n",
    "In other words, knowing the current state is enough to know probabilities for future states.\n",
    "\n",
    "In particular, the dynamics of a Markov chain are fully determined by the set of values\n",
    "\n",
    "$$\n",
    "    P(x, y) := \\mathbb P \\{ X_{t+1} = y \\,|\\, X_t = x \\}\n",
    "    \\qquad (x, y \\in S) \n",
    "$$\n",
    "\n",
    "We can view $ P $ as a stochastic matrix where\n",
    "\n",
    "$$\n",
    "P_{ij} = P(x_i, x_j)\n",
    "\\qquad 1 \\leq i, j \\leq n\n",
    "$$\n",
    "\n",
    "Going the other way, if we take a stochastic matrix $ P $, we can generate a Markov\n",
    "chain $ \\{X_t\\} $ as follows:\n",
    "\n",
    "- draw $ X_0 $ from a marginal distribution $ \\psi $  \n",
    "- for each $ t = 0, 1, \\ldots $, draw $ X_{t+1} $ from $ P(X_t,\\cdot) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33608ef",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Consider a worker who, at any given time $ t $, is either unemployed (state 0) or employed (state 1).\n",
    "\n",
    "Suppose that, over a one month period,\n",
    "\n",
    "1. An unemployed worker finds a job with probability $ \\alpha \\in (0, 1) $.  \n",
    "1. An employed worker loses her job and becomes unemployed with probability $ \\beta \\in (0, 1) $.  \n",
    "\n",
    "\n",
    "In terms of a Markov model, we have\n",
    "\n",
    "- $ S = \\{ 0, 1\\} $  \n",
    "- $ P(0, 1) = \\alpha $ and $ P(1, 0) = \\beta $  \n",
    "\n",
    "\n",
    "We can write out the transition probabilities in matrix form as\n",
    "\n",
    "$$\n",
    "P\n",
    "= \\left(\n",
    "\\begin{array}{cc}\n",
    "    1 - \\alpha & \\alpha \\\\\n",
    "    \\beta      & 1 - \\beta\n",
    "\\end{array}\n",
    "  \\right) \n",
    "$$\n",
    "\n",
    "Once we have the values $ \\alpha $ and $ \\beta $, we can address a range of questions, such as\n",
    "\n",
    "- What is the average duration of unemployment?  \n",
    "- Over the long-run, what fraction of time does a worker find herself unemployed?  \n",
    "\n",
    "We'll see how to do this below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfa254",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "From  US unemployment data, James Hamilton estimated the stochastic matrix\n",
    "\n",
    "$$\n",
    "P =\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "     0.971 & 0.029 & 0 \\\\\n",
    "     0.145 & 0.778 & 0.077 \\\\\n",
    "     0 & 0.508 & 0.492\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* the frequency is monthly  \n",
    "* the first state represents â€œnormal growthâ€  \n",
    "* the second state represents â€œmild recessionâ€  \n",
    "* the third state represents â€œsevere recessionâ€  \n",
    "\n",
    "For example, the matrix tells us that when the state is normal growth, the state will again be normal growth next month with probability 0.97.\n",
    "\n",
    "In general, large values on the main diagonal indicate persistence in $ \\{ X_t \\} $.\n",
    "\n",
    "This Markov process can also be represented as a directed graph, with edges labeled by transition probabilities\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/finite_markov/hamilton_graph.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/hamilton_graph.png)\n",
    "\n",
    "  \n",
    "\n",
    "## Simulation\n",
    "\n",
    "One natural way to study Markov chains is to simulate them.\n",
    "\n",
    "(The LLN tells us that to approximate the probability of event $ E $, we can simulate many times and count the fraction of times that $ E $ occurs).\n",
    "\n",
    "Let's start by writing our own routines for generating sample paths (Markov chains).\n",
    "\n",
    "(Later we'll use routines in [QuantEcon.py](http://quantecon.org/quantecon-py).)\n",
    "\n",
    "In these exercises, weâ€™ll take the state space to be $ S = 0,\\ldots, n-1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1306c",
   "metadata": {},
   "source": [
    "### Rolling Our Own\n",
    "\n",
    "To simulate a Markov chain, we need its stochastic matrix $ P $ and a marginal probability distribution $ \\psi $  from which to  draw a realization of $ X_0 $.\n",
    "\n",
    "The Markov chain is then constructed as discussed above.  To repeat:\n",
    "\n",
    "1. At time $ t=0 $, draw a realization of  $ X_0 $  from $ \\psi $.  \n",
    "1. At each subsequent time $ t $, draw a realization of the new state $ X_{t+1} $ from $ P(X_t, \\cdot) $.  \n",
    "\n",
    "\n",
    "To implement this simulation procedure, we need a method for generating draws from a discrete distribution.\n",
    "\n",
    "For this task, weâ€™ll use `random.draw` from [QuantEcon](http://quantecon.org/quantecon-py), which is accelerated by Numba and works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10efe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïˆ = (0.3, 0.7)           # Probabilities over {0, 1}\n",
    "cdf = np.cumsum(Ïˆ)       # Convert into cummulative distribution\n",
    "qe.random.draw(cdf, 5)   # Generate 5 independent draws from Ïˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a66d7f",
   "metadata": {},
   "source": [
    "Weâ€™ll write our code as a function that accepts the following three arguments\n",
    "\n",
    "- A stochastic matrix `P`  \n",
    "- An initial state `init`  \n",
    "- A positive integer `sample_size` representing the length of the time series the function should return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_sample_path(P, Ïˆ_0=None, sample_size=1_000):\n",
    "    \"\"\"\n",
    "    Generate a sample path of a finite state Markov chain with transition matrix\n",
    "    P and initial distribution Ïˆ_0.\n",
    "\n",
    "\n",
    "    If Ïˆ_0 is set to None then the initial draw will be uniform over the states.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up\n",
    "    P = np.asarray(P)\n",
    "    X = np.empty(sample_size, dtype=int)\n",
    "\n",
    "    # draw initial state, defaulting to 0\n",
    "    if Ïˆ_0 is None:\n",
    "        X_0 = 0\n",
    "    else:\n",
    "        Ïˆ_0 = np.cumsum(Ïˆ_0)\n",
    "        X_0 = qe.random.draw(Ïˆ_0)\n",
    "\n",
    "    # Convert each row of P into a cdf\n",
    "    P = np.cumsum(P, axis=1)\n",
    "\n",
    "    # Simulate\n",
    "    X[0] = X_0\n",
    "    for t in range(sample_size - 1):\n",
    "        X[t+1] = qe.random.draw(P[X[t], :])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08f7d1",
   "metadata": {},
   "source": [
    "Letâ€™s see how it works using the small matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b832c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0.4, 0.6],\n",
    "     [0.2, 0.8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743acfe4",
   "metadata": {},
   "source": [
    "As weâ€™ll see later, for a long series drawn from `P`, the fraction of the sample that takes value 0 should be about 0.25.\n",
    "\n",
    "The following code illustrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X = mc_sample_path(P, Ïˆ_0=(0.1, 0.9), sample_size=1_000_000)\n",
    "np.mean(X == 0)  # Fraction of time that X_t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70a9a1",
   "metadata": {},
   "source": [
    "Here we chose a particular initial condition but the result holds regardless of the initial distribution.\n",
    "\n",
    "You can try changing the initial distribution to confirm this.\n",
    "\n",
    "\n",
    "### Using QuantEconâ€™s Routines\n",
    "\n",
    "[QuantEcon.py](http://quantecon.org/quantecon-py) has routines for handling Markov chains, including simulation.\n",
    "\n",
    "Hereâ€™s an illustration using the same P as the preceding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon import MarkovChain\n",
    "\n",
    "mc = qe.MarkovChain(P)\n",
    "X = mc.simulate(ts_length=1_000_000)\n",
    "np.mean(X == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a721028",
   "metadata": {},
   "source": [
    "The [QuantEcon.py](http://quantecon.org/quantecon-py) routine is [JIT compiled](https://python-programming.quantecon.org/numba.html#numba-link) and hence faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac817ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path(P, sample_size=1_000_000) # Our homemade version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb763e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc.simulate(ts_length=1_000_000)         # QE version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac15e01",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Try producing your own JIT compiled version of our homemade function `mc_sample_path`, using Numba.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- You might need to replace `dtype=int` with `dtype=numba.int32` or similar\n",
    "- To simplify logic, you can replace `Ïˆ_0=None` with `Ïˆ_0` and assume the initial distribution is always passed in.\n",
    "\n",
    "How does the timing compare to the original and the QuantEcon routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(\"Solution below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b611b5",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Here's one solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4032b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def mc_sample_path_fast(P, Ïˆ_0,  sample_size=1_000):\n",
    "\n",
    "    # Set up\n",
    "    P = np.asarray(P)\n",
    "    Ïˆ_0 = np.asarray(Ïˆ_0)\n",
    "    X = np.empty(sample_size, dtype=numba.int64)\n",
    "\n",
    "    # draw initial state\n",
    "    Ïˆ_0 = np.cumsum(Ïˆ_0)\n",
    "    X_0 = qe.random.draw(Ïˆ_0)\n",
    "\n",
    "    # Convert each row of P into a cdf\n",
    "    P_cdf = np.empty_like(P)\n",
    "    for i in range(len(P)):\n",
    "        P_cdf[i, :] = np.cumsum(P[i, :])\n",
    "    P = P_cdf\n",
    "    \n",
    "    # Simulate\n",
    "    X[0] = X_0\n",
    "    for t in range(sample_size - 1):\n",
    "        X[t+1] = qe.random.draw(P[X[t], :])\n",
    "    return X\n",
    "\n",
    "Ïˆ_0 = (1, 0)\n",
    "P = [[0.4, 0.6],\n",
    "     [0.2, 0.8]]\n",
    "P, Ïˆ_0 = np.asarray(P), np.asarray(Ïˆ_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e683d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path_fast(P, Ïˆ_0, sample_size=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77108420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path_fast(P, Ïˆ_0, sample_size=1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb16a6f",
   "metadata": {},
   "source": [
    "Incidentally, we can also hold the stochastic matrix as state in a jitted function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_sample_path_factory(P, Ïˆ_0):\n",
    "\n",
    "    # Set up\n",
    "    P = np.asarray(P)\n",
    "    Ïˆ_0 = np.asarray(Ïˆ_0)\n",
    "\n",
    "    # draw initial state\n",
    "    Ïˆ_0 = np.cumsum(Ïˆ_0)\n",
    "\n",
    "    # Convert each row of P into a cdf\n",
    "    P_cdf = np.empty_like(P)\n",
    "    for i in range(len(P)):\n",
    "        P_cdf[i, :] = np.cumsum(P[i, :])\n",
    "    P = P_cdf\n",
    "\n",
    "    @numba.jit\n",
    "    def mc_sample_path_closure(sample_size=1_000):\n",
    "        X_0 = qe.random.draw(Ïˆ_0)\n",
    "        X = np.empty(sample_size, dtype=numba.int32)\n",
    "        # Simulate\n",
    "        X[0] = X_0\n",
    "        for t in range(sample_size - 1):\n",
    "            X[t+1] = qe.random.draw(P[X[t], :])\n",
    "        return X\n",
    "\n",
    "    return mc_sample_path_closure\n",
    "\n",
    "mc_sample_path_closure = mc_sample_path_factory(P, Ïˆ_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96469ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path_closure(sample_size=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path_closure(sample_size=1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272690d",
   "metadata": {},
   "source": [
    "## Marginal Distributions\n",
    "Suppose that\n",
    "\n",
    "1. $ \\{X_t\\} $ is a Markov chain with stochastic matrix $ P $  \n",
    "1. the marginal distribution of $ X_t $ is known to be $ \\psi_t $  \n",
    "\n",
    "\n",
    "The law of total probability implies that\n",
    "\n",
    "$$\n",
    "\\psi_{t+1}(y) = \\sum_{x \\in S} P(x,y) \\psi_t(x)\n",
    "\\quad \\text{for all } y \\in S.\n",
    "$$\n",
    "\n",
    "If we think of $ \\psi_{t+1} $ and $ \\psi_t $ as *row vectors*, these $ n $ equations are summarized by the matrix expression\n",
    "\n",
    "\n",
    "$$\n",
    "\\psi_{t+1} = \\psi_t P \n",
    "$$\n",
    "\n",
    "Thus, to move a marginal distribution forward one unit of time, we postmultiply by $ P $.\n",
    "\n",
    "Repeating this $m$ times gives\n",
    "\n",
    "$$\n",
    "    X_0 \\sim \\psi_0 \\quad \\implies \\quad X_m \\sim \\psi_0 P^m \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14e3a2",
   "metadata": {},
   "source": [
    "### Example: Probability of Recession\n",
    "\n",
    "Recall Hamilton's stochastic matrix $ P $ for recession and growth considered above\n",
    "\n",
    "Suppose that the current state is unknown â€” perhaps statistics are available only  at the *end* of the current month.\n",
    "\n",
    "We guess that the probability that the economy is in state $ x $ is $ \\psi(x) $.\n",
    "\n",
    "The probability of being in recession (either mild or severe) in 6 months time is given by the inner product\n",
    "\n",
    "$$\n",
    "\\psi P^6\n",
    "\\cdot\n",
    "\\left(\n",
    "  \\begin{array}{c}\n",
    "     0 \\\\\n",
    "     1 \\\\\n",
    "     1\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Let's compute this when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830dfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïˆ = (0.2, 0.4, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b71e03",
   "metadata": {},
   "source": [
    "To compute $P^6$ we use `np.power`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dcc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ((0.971, 0.029, 0.0), \n",
    "     (0.145, 0.778, 0.077), \n",
    "     (0.0,   0.508, 0.492))\n",
    "\n",
    "Ïˆ @ np.power(P, 6) @ (0, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e9876",
   "metadata": {},
   "source": [
    "### Irreducibility\n",
    "\n",
    "Let $ P $ be a fixed stochastic matrix.\n",
    "\n",
    "Two states $ x $ and $ y $ are said to **communicate** with each other if\n",
    "there exist positive integers $ j $ and $ k $ such that\n",
    "\n",
    "$$\n",
    "    P^j(x, y) > 0\n",
    "    \\quad \\text{and} \\quad\n",
    "    P^k(y, x) > 0\n",
    "$$\n",
    "\n",
    "This means that\n",
    "\n",
    "- state $ x $ can eventually be reached  from state $ y $, and  \n",
    "- state $ y $ can eventually  be reached from state $ x $  \n",
    "\n",
    "\n",
    "The stochastic matrix $ P $ is called **irreducible** if all states\n",
    "communicate; that is, if $ x $ and $ y $ communicate for all\n",
    "$ (x, y) $ in $ S \\times S $.\n",
    "\n",
    "For example, consider the following transition probabilities for wealth of a fictitious set of\n",
    "households\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility1.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility1.png)\n",
    "\n",
    "  \n",
    "We can translate this into a stochastic matrix, putting zeros where\n",
    "thereâ€™s no edge between nodes\n",
    "\n",
    "$$\n",
    "P :=\n",
    "\\left(\n",
    "  \\begin{array}{ccc}\n",
    "     0.9 & 0.1 & 0 \\\\\n",
    "     0.4 & 0.4 & 0.2 \\\\\n",
    "     0.1 & 0.1 & 0.8\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Itâ€™s clear from the graph that this stochastic matrix is irreducible: we can  eventually\n",
    "reach any state from any other state.\n",
    "\n",
    "We can also test this using [QuantEcon.py](http://quantecon.org/quantecon-py)â€™s MarkovChain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_1 = [[0.9, 0.1, 0.0],\n",
    "      [0.4, 0.4, 0.2],\n",
    "      [0.1, 0.1, 0.8]]\n",
    "\n",
    "mc = qe.MarkovChain(P_1, ('poor', 'middle', 'rich'))\n",
    "mc.is_irreducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214410e",
   "metadata": {},
   "source": [
    "Hereâ€™s a more pessimistic scenario in which  poor people remain poor forever\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility2.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_irreducibility2.png)\n",
    "\n",
    "  \n",
    "This stochastic matrix is not irreducible, since, for example, rich is not accessible from poor.\n",
    "\n",
    "Letâ€™s confirm this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_2 = [[1.0, 0.0, 0.0],\n",
    "      [0.1, 0.8, 0.1],\n",
    "      [0.0, 0.2, 0.8]]\n",
    "\n",
    "mc = qe.MarkovChain(P_2, ('poor', 'middle', 'rich'))\n",
    "mc.is_irreducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d2db1",
   "metadata": {},
   "source": [
    "Irreducibility is important for understanding long run outcomes.\n",
    "\n",
    "For example, poverty is a life sentence in the second graph but not the first.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "It's also true that an $n \\times n$ stochastic matrix $P$ is irreducible if and only if $\\sum_{i=0}^n P^i$ is everwhere positive.  \n",
    "\n",
    "Write a function that checks irreducibility of given $P$ using this result and test it on $P_1$ and $P_2$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e372df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    print(\"Solution below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925b8b8",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_irreducible(P):\n",
    "    S = np.zeros_like(P)\n",
    "    n = len(P)\n",
    "    A = np.identity(n)\n",
    "    for i in range(n+1):\n",
    "        S += A\n",
    "        A = A @ P\n",
    "    return np.all(S > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_irreducible(P_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_irreducible(P_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84ba87",
   "metadata": {},
   "source": [
    "### Aperiodicity\n",
    "\n",
    "Loosely speaking, a Markov chain is called **periodic** if it cycles in a predictable way, and **aperiodic** otherwise.\n",
    "\n",
    "Hereâ€™s a trivial example with three states\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity1.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity1.png)\n",
    "\n",
    "  \n",
    "The chain cycles with period 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25472f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0, 1, 0],\n",
    "     [0, 0, 1],\n",
    "     [1, 0, 0]]\n",
    "\n",
    "mc = qe.MarkovChain(P)\n",
    "mc.period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac63b0",
   "metadata": {},
   "source": [
    "More formally, the **period** of a state $ x $ is the largest common divisor\n",
    "of a set of integers\n",
    "\n",
    "$$\n",
    "    D(x) := \\{j \\geq 1 : P^j(x, x) > 0\\}\n",
    "$$\n",
    "\n",
    "In the last example, $ D(x) = \\{3, 6, 9, \\ldots\\} $ for every state $ x $, so the period is 3.\n",
    "\n",
    "A stochastic matrix is called **aperiodic** if the period of every state is 1, and **periodic** otherwise.\n",
    "\n",
    "For example, the stochastic matrix associated with the transition probabilities below is periodic because, for example, state $ a $ has period 2\n",
    "\n",
    "![https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity2.png](https://python.quantecon.org/_static/lecture_specific/finite_markov/mc_aperiodicity2.png)\n",
    "\n",
    "We can confirm that the stochastic matrix is periodic with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f626972",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0.0, 1.0, 0.0, 0.0],\n",
    "     [0.5, 0.0, 0.5, 0.0],\n",
    "     [0.0, 0.5, 0.0, 0.5],\n",
    "     [0.0, 0.0, 1.0, 0.0]]\n",
    "\n",
    "mc = qe.MarkovChain(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.is_aperiodic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bf3b5",
   "metadata": {},
   "source": [
    "## Stationary Distributions\n",
    "\n",
    "\n",
    "We know that we can shift a marginal distribution forward one unit of time via postmultiplication by $ P $.\n",
    "\n",
    "Some distributions are invariant under this updating process â€” for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb399eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0.4, 0.6],\n",
    "              [0.2, 0.8]])\n",
    "Ïˆ = (0.25, 0.75)\n",
    "Ïˆ @ P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d2d4a",
   "metadata": {},
   "source": [
    "Such distributions are called **stationary** or **invariant**.\n",
    "\n",
    "\n",
    "Formally, a marginal distribution $ \\psi^* $ on $ S $ is called **stationary** for $ P $ if $ \\psi^* = \\psi^* P $.\n",
    "\n",
    "\n",
    "\n",
    "**Theorem.** Every stochastic matrix $ P $ has at least one stationary distribution.\n",
    "\n",
    "Proof:  This follows directly from the Perron-Frobenius theorem -- alternatively, see [EDTC](https://johnstachurski.net/edtc.html), theorem 4.3.5.\n",
    "\n",
    "What's an example of a stochastic matrix with many stationary distributions?\n",
    "\n",
    "\n",
    "**Theorem.** If $ P $ is both aperiodic and irreducible, then\n",
    "\n",
    "1. $ P $ has exactly one stationary distribution $ \\psi^* $.  \n",
    "1. For any initial marginal distribution $ \\psi_0 $, we have $ \\| \\psi_0 P^t - \\psi^* \\| \\to 0 $ as $ t \\to \\infty $.  \n",
    "\n",
    "\n",
    "For a proof, see, for example, theorem 5.2 of [[Haggstrom02](https://python.quantecon.org/zreferences.html#id135)].\n",
    "\n",
    "(Note that part 1 of the theorem only requires  irreducibility, whereas part 2 requires both irreducibility and aperiodicity)\n",
    "\n",
    "A stochastic matrix that satisfies the conditions of the theorem is sometimes called **uniformly ergodic**.\n",
    "\n",
    "A sufficient condition for aperiodicity and irreducibility is that every element of $ P $ is strictly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174acd45",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Recall our model of the employment/unemployment dynamics.\n",
    "\n",
    "Assuming $ \\alpha \\in (0,1) $ and $ \\beta \\in (0,1) $, the uniform ergodicity condition is satisfied.\n",
    "\n",
    "Let $ \\psi^* = (p, 1-p) $ be the stationary distribution, so that $ p $ corresponds to unemployment (state 0).\n",
    "\n",
    "Using $ \\psi^* = \\psi^* P $ and a bit of algebra yields\n",
    "\n",
    "$$\n",
    "    p = \\frac{\\beta}{\\alpha + \\beta}\n",
    "$$\n",
    "\n",
    "This is, in some sense, a steady state probability of unemployment â€” more about the  interpretation of this below.\n",
    "\n",
    "Not surprisingly it tends to zero as $ \\beta \\to 0 $, and to one as $ \\alpha \\to 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604fcd8",
   "metadata": {},
   "source": [
    "### Calculating Stationary Distributions\n",
    "\n",
    "\n",
    "As discussed above, a particular Markov matrix $ P $ can have many stationary distributions.\n",
    "\n",
    "A fast algorithm for computing all stationary distributions is implemented in [QuantEcon.py](http://quantecon.org/quantecon-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2aa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0.4, 0.6],\n",
    "     [0.2, 0.8]]\n",
    "\n",
    "mc = qe.MarkovChain(P)\n",
    "mc.stationary_distributions  # Show all stationary distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579de0",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d5746",
   "metadata": {},
   "source": [
    "Another option is to regard the system as an eigenvector problem: a vector\n",
    "$ \\psi $ such that $ \\psi = \\psi P $ is a left eigenvector associated\n",
    "with the unit eigenvalue $ \\lambda = 1$.\n",
    "\n",
    "Try writing a function that uses this information to compute the stationary distribution of $P$.\n",
    "\n",
    "Note that you can get the left eigenvectors by computing the ordinary (right) eigenvectors of the transpose of $P$.\n",
    "\n",
    "Eigenvalues and eigenvectors can be obtained via `np.linalg.eig`.\n",
    "\n",
    "In the exercise you can assume that $P$ has only one stationary distribution, and you can test your function using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1655ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0.4, 0.6],\n",
    "              [0.2, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d542404",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    print(\"Solution below! ðŸ¦˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149190b5",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stationary_via_eigenvecs(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution of P using an eigenvector routine.\n",
    "\n",
    "    We use the fact that, for a stochastic matrix, the largest left eigenvalue is 1.0.\n",
    "    The corresponding eigenvector is the stationary distribution.\n",
    "    \"\"\"\n",
    "    P = np.array(P)\n",
    "    out = np.linalg.eig(P.T)\n",
    "    eigvals, eigvecs = out.eigenvalues, out.eigenvectors\n",
    "    i = np.argmax(eigvals)  # index of largest eigenvalue\n",
    "    dominant_eigvec = eigvecs[:, i] \n",
    "    Ïˆ_star = dominant_eigvec / np.sum(dominant_eigvec) # normalize\n",
    "    return Ïˆ_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ïˆ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f95f5d",
   "metadata": {},
   "source": [
    "### Convergence to Stationarity\n",
    "\n",
    "\n",
    "Part 2 of the Markov chain convergence theorem given above tells us that, under\n",
    "the stated conditions, the marginal distribution of $ X_t $ converges to the\n",
    "stationary distribution regardless of the initial condition.\n",
    "\n",
    "This adds considerable authority to our interpretation of $ \\psi^* $ as a stochastic steady state.\n",
    "\n",
    "The convergence in the theorem is illustrated in the next figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = ((0.971, 0.029, 0.000),\n",
    "     (0.145, 0.778, 0.077),\n",
    "     (0.000, 0.508, 0.492))\n",
    "P = np.array(P)\n",
    "\n",
    "Ïˆ = (0.0, 0.2, 0.8)        # Initial condition\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1), zlim=(0, 1),\n",
    "       xticks=(0.25, 0.5, 0.75),\n",
    "       yticks=(0.25, 0.5, 0.75),\n",
    "       zticks=(0.25, 0.5, 0.75))\n",
    "\n",
    "x_vals, y_vals, z_vals = [], [], []\n",
    "for t in range(20):\n",
    "    x_vals.append(Ïˆ[0])\n",
    "    y_vals.append(Ïˆ[1])\n",
    "    z_vals.append(Ïˆ[2])\n",
    "    Ïˆ = Ïˆ @ P\n",
    "\n",
    "ax.scatter(x_vals, y_vals, z_vals, c='r', s=60)\n",
    "ax.view_init(30, 210)\n",
    "\n",
    "mc = qe.MarkovChain(P)\n",
    "Ïˆ_star = mc.stationary_distributions[0]\n",
    "ax.scatter(Ïˆ_star[0], Ïˆ_star[1], Ïˆ_star[2], c='k', s=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c82b3",
   "metadata": {},
   "source": [
    "Here\n",
    "\n",
    "- $ P $ is the stochastic matrix for recession and growth [considered above](#mc-eg2).  \n",
    "- The highest red dot is an arbitrarily chosen initial marginal probability distribution  $ \\psi $, represented as a vector in $ \\mathbb R^3 $.  \n",
    "- The other red dots are the marginal distributions $ \\psi P^t $ for $ t = 1, 2, \\ldots $.  \n",
    "- The black dot is $ \\psi^* $.  \n",
    "\n",
    "\n",
    "You might like to try experimenting with different initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07724f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
