{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf703d26",
   "metadata": {},
   "source": [
    "# Optimal Savings II: Alternative Algorithms\n",
    "\n",
    "-----\n",
    "\n",
    "#### John Stachurski\n",
    "\n",
    "#### Prepared for the CBC Workshop (May 2024)\n",
    "\n",
    "-----\n",
    "\n",
    "In `opt_savings_1.ipynb` we solved a simple version of the household optimal\n",
    "savings problem via value function iteration (VFI) using JAX.\n",
    "\n",
    "In this lecture we tackle exactly the same problem while adding in two\n",
    "alternative algorithms:\n",
    "\n",
    "* optimistic policy iteration (OPI) and\n",
    "* Howard policy iteration (HPI).\n",
    "\n",
    "We will see that both of these algorithms outperform traditional VFI.\n",
    "\n",
    "One reason for this is that the algorithms have good convergence properties.\n",
    "\n",
    "Another is that one of them, HPI, is particularly well suited to pairing with\n",
    "JAX.\n",
    "\n",
    "The reason is that HPI uses a relatively small number of computationally expensive steps,\n",
    "whereas VFI uses a longer sequence of small steps.\n",
    "\n",
    "In other words, VFI is inherently more sequential than HPI, and sequential\n",
    "routines are hard to parallelize.\n",
    "\n",
    "By comparison, HPI is less sequential -- the small number of computationally\n",
    "intensive steps can be effectively parallelized by JAX.\n",
    "\n",
    "This is particularly valuable when the underlying hardware includes a GPU.\n",
    "\n",
    "Details on VFI, HPI and OPI can be found in [this book](https://dp.quantecon.org), for which a PDF is freely available.\n",
    "\n",
    "Here we assume readers have some knowledge of the algorithms and focus on\n",
    "computation.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Uncomment if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd21b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf690d2d",
   "metadata": {},
   "source": [
    "We will use the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ce578",
   "metadata": {},
   "source": [
    "Let's check the GPU we are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941462db",
   "metadata": {},
   "source": [
    "We'll use 64 bit floats to gain extra precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7edc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e014ac9",
   "metadata": {},
   "source": [
    "## Model primitives\n",
    "\n",
    "We start with a namedtuple to store parameters and arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bff786",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ('β', 'R', 'γ', 'w_grid', 'y_grid', 'Q'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24752849",
   "metadata": {},
   "source": [
    "The following code is repeated from `opt_savings_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6dac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_model(R=1.01,                    # Gross interest rate\n",
    "                             β=0.98,                    # Discount factor\n",
    "                             γ=2,                       # CRRA parameter\n",
    "                             w_min=0.01,                # Min wealth\n",
    "                             w_max=5.0,                 # Max wealth\n",
    "                             w_size=150,                # Grid size\n",
    "                             ρ=0.9, ν=0.1, y_size=100): # Income parameters\n",
    "    \"\"\"\n",
    "    A function that takes in parameters and returns parameters and grids \n",
    "    for the optimal savings problem.\n",
    "    \"\"\"\n",
    "    w_grid = jnp.linspace(w_min, w_max, w_size)\n",
    "    mc = qe.tauchen(n=y_size, rho=ρ, sigma=ν)\n",
    "    y_grid, Q = jnp.exp(mc.state_values), jax.device_put(mc.P)\n",
    "    return Model(β, R, γ, w_grid, y_grid, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07eab2",
   "metadata": {},
   "source": [
    "Here's the right hand side of the Bellman equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _B(v, model, i, j, ip):\n",
    "    \"\"\"\n",
    "    The right-hand side of the Bellman equation before maximization, which takes\n",
    "    the form\n",
    "\n",
    "        B(w, y, w′) = u(Rw + y - w′) + β Σ_y′ v(w′, y′) Q(y, y′)\n",
    "\n",
    "    The indices are (i, j, ip) -> (w, y, w′).\n",
    "    \"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w, y, wp  = w_grid[i], y_grid[j], w_grid[ip]\n",
    "    c = R * w + y - wp\n",
    "    EV = jnp.sum(v[ip, :] * Q[j, :]) \n",
    "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -jnp.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f3996",
   "metadata": {},
   "source": [
    "Now we successively apply `vmap` to vectorize $B$ by simulating nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_vmap = jax.vmap(_B,      in_axes=(None, None, None, None, 0))\n",
    "B_vmap = jax.vmap(B_vmap, in_axes=(None, None, None, 0,    None))\n",
    "B_vmap = jax.vmap(B_vmap, in_axes=(None, None, 0,    None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918b426",
   "metadata": {},
   "source": [
    "Here's a fully vectorized version of $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def B(v, model):\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w_size, y_size = len(w_grid), len(y_grid)\n",
    "    w_indices, y_indices = jnp.arange(w_size), jnp.arange(y_size)\n",
    "    return B_vmap(v, model, w_indices, y_indices, w_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddffd84",
   "metadata": {},
   "source": [
    "## Operators\n",
    "\n",
    "\n",
    "Here's the Bellman operator $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48924308",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v, model):\n",
    "    \"The Bellman operator.\"\n",
    "    return jnp.max(B(v, model), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b76903",
   "metadata": {},
   "source": [
    "The next function computes a $v$-greedy policy given $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v, modeld):\n",
    "    \"Computes a v-greedy policy, returned as a set of indices.\"\n",
    "    return jnp.argmax(B(v, model), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1636b093",
   "metadata": {},
   "source": [
    "We define a function to compute the current rewards $r_\\sigma$ given policy $\\sigma$,\n",
    "which is defined as the vector\n",
    "\n",
    "$$\n",
    "    r_\\sigma(w, y) := r(w, y, \\sigma(w, y)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd46b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_r_σ(σ, model, i, j):\n",
    "    \"\"\"\n",
    "    With indices (i, j) -> (w, y) and wp = σ[i, j], compute \n",
    "        \n",
    "        r_σ[i, j] = u(Rw + y - wp)   \n",
    "\n",
    "    which gives current rewards under policy σ.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    # Compute r_σ[i, j]\n",
    "    w, y, wp = w_grid[i], y_grid[j], w_grid[σ[i, j]]\n",
    "    c = R * w + y - wp\n",
    "    r_σ = c**(1-γ)/(1-γ)\n",
    "\n",
    "    return r_σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c27522",
   "metadata": {},
   "source": [
    "Now we successively apply `vmap` to simulate nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_r_σ_vmap = jax.vmap(_compute_r_σ,     in_axes=(None, None, None, 0))\n",
    "compute_r_σ_vmap = jax.vmap(compute_r_σ_vmap, in_axes=(None, None, 0,    None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4934f",
   "metadata": {},
   "source": [
    "Here's a fully vectorized version of $r_\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_r_σ(σ, model):\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w_size, y_size = len(w_grid), len(y_grid)\n",
    "    w_indices, y_indices = jnp.arange(w_size), jnp.arange(y_size)\n",
    "    return compute_r_σ_vmap(σ, model, w_indices, y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956f031",
   "metadata": {},
   "source": [
    "Now we define the policy operator $T_\\sigma$ going through similar steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _T_σ(v, σ, model, i, j):\n",
    "    \"The σ-policy operator.\"\n",
    "\n",
    "    # Unpack model\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "\n",
    "    r_σ  = _compute_r_σ(σ, model, i, j)\n",
    "    # Calculate the expected sum Σ_jp v[σ[i, j], jp] * Q[i, j, jp]\n",
    "    EV = jnp.sum(v[σ[i, j], :] * Q[j, :])\n",
    "\n",
    "    return r_σ + β * EV\n",
    "\n",
    "\n",
    "T_σ_vmap = jax.vmap(_T_σ,     in_axes=(None, None, None, None, 0))\n",
    "T_σ_vmap = jax.vmap(T_σ_vmap, in_axes=(None, None, None, 0,    None))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def T_σ(v, σ, model):\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w_size, y_size = len(w_grid), len(y_grid)\n",
    "    w_indices, y_indices = jnp.arange(w_size), jnp.arange(y_size)\n",
    "    return T_σ_vmap(v, σ, model, w_indices, y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7106c",
   "metadata": {},
   "source": [
    "The function below computes the value $v_\\sigma$ of following policy $\\sigma$.\n",
    "\n",
    "This lifetime value is a function $v_\\sigma$ that satisfies\n",
    "\n",
    "$$\n",
    "v_\\sigma(w, y) = r_\\sigma(w, y) + \\beta \\sum_{y'} v_\\sigma(\\sigma(w, y), y') Q(y, y')\n",
    "$$\n",
    "\n",
    "We wish to solve this equation for $v_\\sigma$.\n",
    "\n",
    "Suppose we define the linear operator $L_\\sigma$ by\n",
    "\n",
    "$$ \n",
    "(L_\\sigma v)(w, y) = v(w, y) - \\beta \\sum_{y'} v(\\sigma(w, y), y') Q(y, y')\n",
    "$$\n",
    "\n",
    "With this notation, the problem is to solve for $v$ via\n",
    "\n",
    "$$\n",
    "(L_{\\sigma} v)(w, y) = r_\\sigma(w, y)\n",
    "$$\n",
    "\n",
    "In vector for this is $L_\\sigma v = r_\\sigma$, which tells us that the function\n",
    "we seek is\n",
    "\n",
    "$$ \n",
    "v_\\sigma = L_\\sigma^{-1} r_\\sigma \n",
    "$$\n",
    "\n",
    "JAX allows us to solve linear systems defined in terms of operators; the first\n",
    "step is to define the function $L_{\\sigma}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49aecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _L_σ(v, σ, model, i, j):\n",
    "    \"\"\"\n",
    "    Here we set up the linear map v -> L_σ v, where \n",
    "\n",
    "        (L_σ v)(w, y) = v(w, y) - β Σ_y′ v(σ(w, y), y′) Q(y, y′)\n",
    "\n",
    "    \"\"\"\n",
    "    # Unpack\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    # Compute and return v[i, j] - β Σ_jp v[σ[i, j], jp] * Q[j, jp]\n",
    "    return v[i, j]  - β * jnp.sum(v[σ[i, j], :] * Q[j, :])\n",
    "\n",
    "L_σ_vmap = jax.vmap(_L_σ,     in_axes=(None, None, None, None, 0))\n",
    "L_σ_vmap = jax.vmap(L_σ_vmap, in_axes=(None, None, None, 0,    None))\n",
    "\n",
    "@jax.jit\n",
    "def L_σ(v, σ, model):\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    w_size, y_size = len(w_grid), len(y_grid)\n",
    "    w_indices, y_indices = jnp.arange(w_size), jnp.arange(y_size)\n",
    "    return L_σ_vmap(v, σ, model, w_indices, y_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408da944",
   "metadata": {},
   "source": [
    "Now we can define a function to compute $v_{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_value(σ, model):\n",
    "    \"Get the value v_σ of policy σ by inverting the linear map L_σ.\"\n",
    "\n",
    "    r_σ = compute_r_σ(σ, model)\n",
    "    partial_L_σ = lambda v: L_σ(v, σ, model)\n",
    "    return jax.scipy.sparse.linalg.bicgstab(partial_L_σ, r_σ)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7223e",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "\n",
    "We use successive approximation for VFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_approx_jax(T,                     # Operator (callable)\n",
    "                          x_0,                   # Initial condition                \n",
    "                          tol=1e-6,              # Error tolerance\n",
    "                          max_iter=10_000):      # Max iteration bound\n",
    "    def update(inputs):\n",
    "        k, x, error = inputs\n",
    "        x_new = T(x)\n",
    "        error = jnp.max(jnp.abs(x_new - x))\n",
    "        return k + 1, x_new, error\n",
    "\n",
    "    def condition_function(inputs):\n",
    "        k, x, error = inputs\n",
    "        return jnp.logical_and(error > tol, k < max_iter)\n",
    "\n",
    "    k, x, error = jax.lax.while_loop(condition_function, \n",
    "                                     update, \n",
    "                                     (1, x_0, tol + 1))\n",
    "    return x\n",
    "\n",
    "successive_approx_jax = jax.jit(successive_approx_jax, static_argnums=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ead85",
   "metadata": {},
   "source": [
    "For OPI we'll add a compiled routine that computes $T_σ^m v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def iterate_policy_operator(σ, v, m, model):\n",
    "\n",
    "    def update(i, v):\n",
    "        v = T_σ(v, σ, model)\n",
    "        return v\n",
    "    \n",
    "    v = jax.lax.fori_loop(0, m, update, v)\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef9fd8",
   "metadata": {},
   "source": [
    "## Solvers\n",
    "\n",
    "Now we define the solvers, which implement VFI, HPI and OPI.\n",
    "\n",
    "Here's VFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function_iteration(model, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Implements value function iteration.\n",
    "    \"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    sizes = len(w_grid), len(y_grid)\n",
    "    vz = jnp.zeros(sizes)\n",
    "    _T = lambda v: T(v, model)\n",
    "    v_star = successive_approx_jax(_T, vz, tol=tol)\n",
    "    return get_greedy(v_star, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a4535",
   "metadata": {},
   "source": [
    "For OPI we will use a compiled JAX `lax.while_loop` operation to speed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb48af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opi_loop(model, m, tol, max_iter):\n",
    "    \"\"\"\n",
    "    Implements optimistic policy iteration (see dp.quantecon.org) with \n",
    "    step size m.\n",
    "\n",
    "    \"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    sizes = len(w_grid), len(y_grid)\n",
    "    v_init = jnp.zeros(sizes)\n",
    "\n",
    "    def condition_function(inputs):\n",
    "        i, v, error = inputs\n",
    "        return jnp.logical_and(error > tol, i < max_iter)\n",
    "\n",
    "    def update(inputs):\n",
    "        i, v, error = inputs\n",
    "        last_v = v\n",
    "        σ = get_greedy(v, model)\n",
    "        v = iterate_policy_operator(σ, v, m, model)\n",
    "        error = jnp.max(jnp.abs(v - last_v))\n",
    "        i += 1\n",
    "        return i, v, error\n",
    "\n",
    "    num_iter, v, error = jax.lax.while_loop(condition_function,\n",
    "                                            update,\n",
    "                                            (0, v_init, tol + 1))\n",
    "\n",
    "    return get_greedy(v, model)\n",
    "\n",
    "opi_loop = jax.jit(opi_loop, static_argnums=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ce6c9",
   "metadata": {},
   "source": [
    "Here's a friendly interface to OPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc210af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_policy_iteration(model, m=10, tol=1e-4, max_iter=10_000):\n",
    "    σ_star = opi_loop(model, m, tol, max_iter)\n",
    "    return σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a095b7",
   "metadata": {},
   "source": [
    "Here's HPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe342a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def howard_policy_iteration(model, tol=1e-4, maxiter=250):\n",
    "    \"\"\"\n",
    "    Implements Howard policy iteration (see dp.quantecon.org)\n",
    "    \"\"\"\n",
    "    β, R, γ, w_grid, y_grid, Q = model\n",
    "    sizes = len(w_grid), len(y_grid)\n",
    "    v_σ = jnp.zeros(sizes)\n",
    "    i, error = 0, 1.0\n",
    "    while error > tol and i < maxiter:\n",
    "        σ = get_greedy(v_σ, model)\n",
    "        v_σ_new = get_value(σ, model)\n",
    "        error = jnp.max(jnp.abs(v_σ_new - v_σ))\n",
    "        v_σ = v_σ_new\n",
    "        i = i + 1\n",
    "        print(f\"Concluded loop {i} with error {error}.\")\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed2d13",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Create a model for consumption, perform policy iteration, and plot the resulting optimal policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c78802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()\n",
    "β, R, γ, w_grid, y_grid, Q = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_star = howard_policy_iteration(model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_grid, w_grid, \"k--\", label=\"45\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, 1]], label=\"$\\\\sigma^*(\\cdot, y_1)$\")\n",
    "ax.plot(w_grid, w_grid[σ_star[:, -1]], label=\"$\\\\sigma^*(\\cdot, y_N)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b470991",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Let's create an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb282d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_consumption_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d0dea",
   "metadata": {},
   "source": [
    "Here's a function that runs any one of the algorithms and returns the result and\n",
    "elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(algorithm, model, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = algorithm(model, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{algorithm.__name__} completed in {elapsed_time:.2f} seconds.\")\n",
    "    return result, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9d5fc",
   "metadata": {},
   "source": [
    "Here's a quick test of each model.\n",
    "\n",
    "HPI first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730503b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_pi, pi_time = run_algorithm(howard_policy_iteration, \n",
    "                              model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c541e",
   "metadata": {},
   "source": [
    "HPI second run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119395be",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_pi, pi_time = run_algorithm(howard_policy_iteration, \n",
    "                              model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f36c9",
   "metadata": {},
   "source": [
    "VFI first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting VFI.\")\n",
    "σ_vfi, vfi_time = run_algorithm(value_function_iteration, \n",
    "                                model, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f9674",
   "metadata": {},
   "source": [
    "VFI second run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424188ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting VFI.\")\n",
    "σ_vfi, vfi_time = run_algorithm(value_function_iteration, \n",
    "                                model, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e1b13",
   "metadata": {},
   "source": [
    "OPI first run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "print(f\"Starting OPI with $m = {m}$.\")\n",
    "σ_opi, opi_time = run_algorithm(optimistic_policy_iteration, \n",
    "                                model, m=m, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43946097",
   "metadata": {},
   "source": [
    "OPI second run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb66369",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "print(f\"Starting OPI with $m = {m}$.\")\n",
    "σ_opi, opi_time = run_algorithm(optimistic_policy_iteration, \n",
    "                                model, m=m, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b1688",
   "metadata": {},
   "source": [
    "Now let's run OPI at a range of $m$ values and plot the execution time along\n",
    "side the execution time for VFI and HPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47890cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ_pi, pi_time = run_algorithm(howard_policy_iteration, model)\n",
    "σ_vfi, vfi_time = run_algorithm(value_function_iteration, model, tol=1e-4)\n",
    "m_vals = range(5, 600, 40)\n",
    "opi_times = []\n",
    "for m in m_vals:\n",
    "    σ_opi, opi_time = run_algorithm(optimistic_policy_iteration, \n",
    "                                    model, m=m, tol=1e-4)\n",
    "    opi_times.append(opi_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74d0e8",
   "metadata": {},
   "source": [
    "Here's the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb4663",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(m_vals, \n",
    "        jnp.full(len(m_vals), pi_time), \n",
    "        lw=2, label=\"Howard policy iteration\")\n",
    "ax.plot(m_vals, \n",
    "        jnp.full(len(m_vals), vfi_time), \n",
    "        lw=2, label=\"value function iteration\")\n",
    "ax.plot(m_vals, opi_times, \n",
    "        lw=2, label=\"optimistic policy iteration\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlabel(\"$m$\")\n",
    "ax.set_ylabel(\"time\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
